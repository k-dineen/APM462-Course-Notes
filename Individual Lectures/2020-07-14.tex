\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, amsfonts, mathtools, tikz-cd, float}
\usepackage[left=2.5cm,right=2.5cm]{geometry}
\usepackage[shortlabels]{enumitem}

\newcommand{\Int}{\text{Int}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\pd}{\partial}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\hat}{\widehat}
\renewcommand{\tilde}{\widetilde}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}

\pagestyle{myheadings}


\begin{document}

\section{Steepest Descent Convergence, Conjugate Directions (July 14)}

\subsection{Recap}

Consider $f(x) = \frac{1}{2}x^TQx - b^Tx$, where $Q$ is positive definite symmetric, and has eigenvalues $\lambda = \lambda_1 \leq \cdots \leq \lambda_n = \Lambda$. Since $Q$ is positive definite, there is a unique minimizer $x_*$ such that $Qx_* = b$. Let $g(x) = \nabla f(x) = Qx - b$. We may as well minimize $q(x) = \frac{1}{2}(x-x_*)^TQ(x-x_*) = f(x) + \mathrm{const}$. Moreover, $q$ is always positive except at $x = x_*$, so $q$ is nicer to work with. Note that $\nabla q(x) = \nabla f(x) = g(x) = Qx - b$. Denote by $g_k$ the point $g(x_k) = Qx_k - b$. Then, if $x_k$ is generated by steepest descent, we derived the expression
\[
q(x_{k+1}) = \left( 1 - \frac{|g_k|^4}{(g_k^TQg_k)(g_k^TQ^{-1}g_k)} \right)q(x_k).
\]
We may use this to study the rate of convergence of gradient descent.

\subsection{Rate of Convergence of Steepest Descent} 

If $v = g_k$, then the term in the brackets may be written
\[
1 - \frac{|v|^4}{(v^TQv)(v^TQ^{-1}v)}.
\]
\emph{Kantorovich's inequality} says that if $Q$ is an $n \times n$ positive definite symmetric matrix with eigenvalues $\lambda = \lambda_1 \leq \cdots \leq \lambda_n = \Lambda$, then
\[
\frac{|v|^4}{(v^T Q v)(v^T Q^{-1} v)} \geq \frac{4\lambda\Lambda}{(\lambda+\Lambda)^2} \qquad \text{ for all } v \in \R^n.
\]
Thus
\[
q(x_{k+1}) = \left( 1 - \frac{|v|^4}{(v^TQv)(v^TQ^{-1}v)} \right)q(x_k) \leq \left( 1 - \frac{4\lambda\Lambda}{(\lambda+\Lambda)^2} \right) q(x_k),
\]
which simplifies to, after some work,
\[
q(x_{k+1}) \leq \underbrace{\frac{(\lambda - \Lambda)^2}{(\lambda + \Lambda)^2}}_{r} q(x_k).
\]
Then $0 \leq r < 1$. We shall call the constant $r$ the \emph{rate of convergence}. We state some properties of steepest descent in the quadratic case. The only thing we have to prove in the following theorem is that steepest descent converges.

\begin{theorem}
(Steepest descent, quadratic case) For $x_0 \in \R^n$, the method of steepest descent starting at $x_0$ converges to the unique minimizer $x_*$ of the function $f$, and we have $q(x_{k+1}) \leq r q(x_k)$.
\end{theorem}
\begin{proof}
We know that $q(x_{k+1}) \leq r^k q(x_0)$. Since $0 \leq r < 1$, when $k \to \infty$, $r^k \to 0$. Note that
\[
x_k \in \{ x \in \R^n : q(x) \leq r^k q(x_0) \}.
\]
This set is a sublevel set of $q$. The sublevel sets of $q$ look like concentric filled-in ellipses centred at $x_*$, and as $k \to \infty$, they seem to "shrink" into $x_*$. Therefore steepest descent converges in the quadratic case.
\end{proof}

Note that
\[
r = \frac{(\Lambda - \lambda)^2}{(\Lambda - \lambda)^2} = \frac{( \Lambda / \lambda - 1 )^2}{(\Lambda / \lambda - 1)^2},
\]
so $r$ depends only on the ratio $\Lambda / \lambda$. This number is called the \emph{condition number of $Q$}. (The condition number may be defined as $\|Q\|\|Q^{-1}\|$ in the operator norm on matrices; it is not hard to see that these numbers agree in our case.) 

If the condition number $\Lambda / \lambda \gg 1$ (large), then convergence is very slow. If $\Lambda / \lambda = 1$, then $r = 0$, and so convergence is achieved in one step.

\subsection{Method of Conjugate Directions}

We will develop a new method for finding the minimizers of quadratic functions $\frac{1}{2}x^TQx - b^Tx$.

\begin{definition}
Let $Q$ be symmetric. We say that $d,d'$ are $Q$-conjugate or $Q$-orthogonal if $d^TQd' = 0$. A finite set $d_0, \dots, d_k$ of vectors is called $Q$-orthogonal if $d_i^TQd_j = 0$ for all $i \geq j$.
\end{definition}

For example, if $Q = I$, then $Q$-orthogonality is equivalent to regular orthogonality. For another example, if $Q$ has more than one distinct eigenvalue, let $d$ and $d'$ be eigenvectors corresponding to distinct eigenvalues. Then $d^TQd' = \lambda' d^Td' = 0$, since the distinct eigenspaces of a symmetric matrix are orthogonal subspaces.

Recall that any symmetric matrix $Q$ may the orthogonally diagonalized; there exists an orthonormal basis $d_0, \dots, d_{n-1}$ of eigenvectors of $Q$. These eigenvectors are also $Q$-orthogonal. Hence to any symmetric matrix is a basis of orthonormal vectors that are also orthogonal with respect to the matrix, as just defined.

\begin{proposition}
If $Q$ is symmetric and positive definite, then any set of non-zero $Q$-orthogonal vectors $\{d_i\}$ is linearly independent.
\end{proposition}
\begin{proof}
If $\sum \alpha_i d_i = 0$, then left-multiplying by $d_j^TQ$ gives $\alpha_j d_j^T Q d_j = 0$. Positive definiteness implies $\alpha_j = 0$. 
\end{proof}

Let $Q$ be an $n \times n$ symmetric positive definite matrix. Recall that $f(x) = \frac{1}{2}x^TQx - b^Tx$ has the unique global minimizer $x_* = Q^{-1}b$. Let $d_0, \dots, d_{n-1}$ be non-zero $Q$-orthogonal vectors. Then $d_0, \dots, d_{n-1}$ form a basis of $\R^n$. Thus there are scalars $\alpha_0, \dots, \alpha_{n-1}$ such that $x_* = \sum \alpha_i d_i$. We would like a formula for the $\alpha_i$'s.

Multiplying both sides of the sum $x_* = \sum \alpha_i d_i$ by $d_j^TQ$ implies that $d_j^TQx_* = \alpha_j d_j^TQd_j$, implying that
\[
\alpha_j = \frac{d_j^T b}{d_j^TQd_j}.
\]
Therefore
\[
x_* = \sum_{i=1}^{n-1} \frac{d_i^Tb}{d_i^TQd_i}  d_i.
\]
This implies that we can actually solve for $x_*$ by computing the $d_0, \dots, d_{n-1}$ and the coefficients above. Computationally, computing inner products is very easy. The disadvantage is that we do not know how to find the vectors $d_0, \dots, d_{n-1}$.

%what the hell is this
\begin{theorem}
(Method of Conjugate Directions)
Let $d_0, \dots, d_{n-1}$ be a set of non-zero $Q$-orthogonal vectors. For a starting point $x_0 \in \R^n$, consider the sequence $\{x_l\}$ defined by 
\[
x_{k+1} = x_k + \alpha_k d_k,
\]
where
\[
\alpha_k = -\frac{g_k^Td_k}{d_k^TQd_k} \qquad \text{ where } g_k = Qx_k - b.
\]
The sequence $\{x_k\}$ converges to the minimizer $x_*$ it at most $n$ steps; $x_n = x_*$.
\end{theorem}
\begin{proof}
Write $x_* - x_0 = \alpha_0' d_0 + \cdots + \alpha_{n-1}'d_{n-1}$. Multiply both sides by $d_i^TQ$ to get
\[
d_i^TQ(x_* - x_0) = \alpha_i d_i^TQd_i,
\]
giving us the expression
\[
\tag{*}
\alpha_i' = \frac{d_i^TQ(x_*-x_0)}{d_i^TQd_i}.
\]
Note that
\begin{align*}
x_1 &= x_0 + \alpha_0 d_0 \\
x_2 &= x_0 + \alpha_0 d_0 + \alpha_1 d_1 \\
&\vdots \\
x_k &= x_0 + \alpha_0 d_0 + \cdots + \alpha_{k-1}d_{k-1},
\end{align*}
implying that
\[
x_k - x_0 = \alpha_0 d_0 + \cdots + \alpha_{k-1}d_{k-1}.
\]
Multiplying both sides by $d_k^TQ$ gives $d_k^TQ(x_k-x_0) = 0$. By (*) we have
\[
\alpha_k' = \frac{d_k^T Q(x_* - x_0) - d_k^TQ(x_k - x_0)}{d_k^TQd_k} = \frac{d_k^TQ(x_* - x_k)}{d_k^TQd_k} = -\frac{(Qx_k - Qx_*)^T d_k}{d_k^TQd_k}
\]
simplifying to
\[
\alpha_k' = -\frac{g_k^T d_k}{d_k^TQd_k} = \alpha_k.
\]
So
\[
x_* = x_0 + \alpha_0 d_0 + \cdots + \alpha_{n-1}d_{n-1} = x_n.
\]
So after $n$ steps, we reach the minimizer.
\end{proof}
\textbf{(There may be an error in the above calculations. The professor will send a note on this.)}

\subsection{Geometric Interpretation of Conjugate Directions}

Let $d_0, \dots, d_{n-1}$ be a set of non-zero $Q$-orthogonal vectors in $\R^n$. Let $B_k$ be the span of the first $k$ of these vectors. Note that $B_k$ has dimension $k$ and contains $B_1, \dots, B_{k-1}$, so $B_1, \dots, B_n$ is a sequence of expanding subspaces of $\R^n$. Let us agree that $B_0 = \{0\}$.

Fix $x_0 \in \R^n$ and consider the affine subspaces $x_0 + B_k$ each with "origin" $x_0$. We now have a sequence of expanding affine subspaces of $\R^n$.

\begin{theorem}
The sequence $\{x_k\}$ generated from $x_0$ by the method of conjugate directions has the property that $x_k$ is the minimizer of $f(x) = \frac{1}{2}x^TQx - b^Tx$ on the affine subspace $x_0 + B_k$.
\end{theorem}

\end{document}
 