\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, amsfonts, mathtools, tikz-cd, float}
\usepackage[hidelinks]{hyperref}
\usepackage[left=2.5cm,right=2.5cm]{geometry}
\usepackage[shortlabels]{enumitem}

\hypersetup{linktoc=all}

\newcommand{\Int}{\text{Int}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\pd}{\partial}
\renewcommand{\epsilon}{\varepsilon}

% HOW TO READ THESE
% (definiton/theory/corollary/lemma) a.b.c is the cth respective object of section a, subsection b

\newtheorem{definition}{Definition}[subsection]
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{corollary}{Corollary}[subsection]
\newtheorem{lemma}{Lemma}[subsection]

\pagestyle{myheadings}
\title{APM462 Course Notes}
\author{Kain Dineen}

\begin{document}
\maketitle

The following are lecture notes for APM462 (Nonlinear Optimization) offered in the Summer of 2020, taught by Jonathan Korman. These lecture notes were typed during lectures, and are not based off of any handwritten notes. These notes were created three weeks in to the course, and do not (as of now) include the material from the first two weeks.

\tableofcontents

\newpage
\section{Unconstrained Finite-Dimensional Optimization (May 19)}

\subsection{First Order Necessary Condition}

Our main problem is
\begin{align*}
&\min_{x \in \Omega} f(x) \qquad f : \R^n \supseteq \Omega \to \R,
\end{align*}
where $\Omega$ is one of the following three types:
\begin{itemize}
\item $\Omega = \R^n$.
\item $\Omega$ open.
\item $\Omega$ the closure of an open set.
\end{itemize}
We can consider minimization problems without any loss of generality, since any maximization problem can be converted to a minimization problem by taking the negative of the function in question: that is,
\[
\max_{x \in \Omega} f(x) = \min_{x \in \Omega} -f(x).
\]
\begin{definition}
Given $\Omega \subseteq \R^n$ and a point $x_0 \in \Omega$, we say that the vector $v \in \R^n$ is a feasible direction at $x_0$ if there is an $\overline{s} > 0$ such that $x_0 + sv \in \Omega$ for all $s \in [0, \overline{s}]$.
\end{definition}
\begin{theorem}
(First order necessary condition for a local minimum, or FONC) Let $f : \R^n \supseteq \Omega \to \R$ be $C^1$. If $x_0 \in \Omega$ is a local minimizer of $f$, then $\nabla f(x_0) \cdot v \geq 0$ for all feasible directions $v$ at $x_0$.
\end{theorem}
First we deduce a familiar case of the theorem - the one we know from second-year calculus.
\begin{corollary}
If $f : \R^n \supseteq \Omega \to \R$ is $C^1$ and $x_0$ is a local minimizer of $f$ in the interior of $\Omega$, then $\nabla f(x_0) = 0$.
\end{corollary}
\begin{proof}
If $x_0$ is an interior point of $\Omega$, then all directions at $x_0$ are feasible. In particular, for any such $v$, we have $\nabla f(x_0) \cdot (v) \geq 0$ and $\nabla f(x_0) \cdot (-v) \geq 0$, which implies $\nabla f(x_0) = 0$ as all directions are feasible at $x_0$.
\end{proof}
Now we prove the theorem.
\begin{proof}
Reduce to a single-variable problem by defining $g(s) = f(x_0 + sv)$, where $s \geq 0$. Then $0$ is a local minimizer of $g$. Taylor's theorem gives us
\[
g(s) - g(0) = s g'(0) + o(s) = s \nabla f(x_0) \cdot v + o(s).
\]
If $\nabla f(x_0) \cdot v < 0$, then for sufficiently small $s$ the right side is negative. This implies that $g(s) < g(0)$ for those $s$, a contradiction. Therefore $\nabla f(x_0) \cdot v \geq 0$.
\end{proof}

\subsection{Examples of using the FONC}
\begin{enumerate}
\item 
Consider the problem
\begin{align*}
\min_{x \in \Omega} f(x,y) = x^2 - xy + y^2 - 3y \qquad \text{over } \Omega = \R^2.
\end{align*}
By the corollary to the FONC, we want to find the points $(x_0, y_0)$ where $\nabla f(x_0, y_0) = 0$. We have
\begin{align*}
\nabla f(x,y) = (2x-y, -x+2y-3),
\end{align*}
so we want to solve 
\begin{align*}
2x - y &= 0 \\
-x + 2y &= 3,
\end{align*}
which has solution $(x_0, y_0) = (1,2)$. Therefore $(1,2)$ is the only \emph{candidate} for a local minimizer. That is, if the function $f$ has a local minimizer in $\R^2$, then it must be $(1,2)$.

It turns out that $(1,2)$ is a global minimizer for $f$ on $\Omega = \R^2$. By some work, we have
\[
f(x,y) = \left(x - \frac{y}{2}\right)^2 + \frac{3}{4}(y-2)^2 - 3.
\]
In this form, it is obvious that a \emph{global} minimizer occurs at the point where the squared terms are zero, if such a point exists. That point is $(1,2)$.

\item
Consider the problem
\begin{align*}
\min_{x \in \Omega} f(x,y) = x^2 - x + y + xy \qquad \text{over } \Omega = \{(x,y) \in \R^2 : x,y \geq 0\}.
\end{align*}
We have
\[
\nabla f(x,y) = (2x + y - 1, x + 1).
\]
To apply the FONC, we'll divide the feasible set $\Omega$ into four different regions. Suppose that $(x_0, y_0)$ is a local minimizer of $f$ on $\Omega$.
\begin{enumerate}[(i)]
\item $(x_0, y_0)$ is an interior point: 

By the corollary to the FONC, we must have $\nabla f(x_0, y_0) = 0$. Then $x_0 = -1$, which is not in the interior of $\Omega$. This case fails.

\item $(x_0, y_0)$ on the positive x-axis: 

Then we are considering $(x_0, 0)$. The feasible directions at $(x_0, 0)$ are those vectors $v \in \R^2$ with $v_2 \geq 0$. The FONC tells us that $\nabla f(x_0,0) \cdot v \geq 0$ for all feasible directions $v$. We then have
\[
(2x_0 - 1)v_1 + (x_0 + 1)v_2 \geq 0
\]
for all $v_1$ and all $v_2 \geq 0$. In particular, this holds for $v_2 = 0$, so $(2x_0 - 1)v_1 \geq 0$ for all $v_1$, implying $x_0 = 1/2$. Therefore $(1/2, 0)$ is a candidate for a local minimizer of $f$ on $\Omega$ - this is the only candidate for a local minimizer of $f$ on the positive $x$-axis.

\item $(x_0, y_0)$ on the positive y-axis:

Then we are considering $(0, y_0)$. The feasible directions here are $v \in \R^2$ with $v_1 \geq 0$. Then we have
\[
(y_0 - 1)v_1 + v_2 \geq 0
\]
for any $v_2$ and $v_1 \geq 0$. This is a contradiction if we take $v_1 = 0$, so $f$ has no local minimizers along the positive $y$-axis.

\item $(x_0, y_0)$ is the origin:

Then we are considering $(0,0)$. The feasible directions here are $v \in \R^2$ with $v_1, v_2 \geq 0$. Then we have
\[
-v_1 + v_2 \geq 0
\]
for all $v_1, v_2 \geq 0$, a contradiction. Therefore the origin is not a local minimizer of $f$.
\end{enumerate}
We conclude that the only candidate for a local minimizer of $f$ is $(1/2, 0)$. It turns out that this is actually a global minimizer of $f$ on $\Omega$. (This is to be seen.)
\end{enumerate}

\subsection{Second Order Necessary Condition}

\begin{theorem}
(Second order necessary condition for a local minimum, or SONC) Let $f : \R^n \supseteq \Omega \to \R$ be $C^2$. If $x_0 \in \Omega$ is a local minimizer of $f$, then for any feasible direction $v$ at $x_0$ the following conditions hold:
\begin{enumerate}[(i)]
\item $\nabla f(x_0) \cdot v \geq 0$.
\item If $\nabla f(x_0) \cdot v = 0$, then $v^T \nabla^2 f(x_0) v \geq 0$.
\end{enumerate}
\end{theorem}
\begin{proof}
Fix a feasible direction $v$ at $x_0$. Then $f(x_0) \leq f(x_0 + sv)$ for sufficiently small $s$. By Taylor's theorem,
\[
f(x_0 + sv) = f(x_0) + s \nabla f(x_0) + \frac{1}{2} s^2 v^T \nabla^2 f(x_0) v + o(s^2),
\]
so by the FONC,
\[
f(x_0 + sv) - f(x_0) = \frac{1}{2} s^2 v^T \nabla^2 f(x_0) v + o(s^2).
\]
If $v^T \nabla^2 f(x_0) v < 0$, then for sufficiently small $s$ the right side is negative, implying that $f(x_0 + sv) < f(x_0)$ for such $s$, which contradicts local minimality of $f(x_0)$. Therefore $v^T \nabla^2 f(x_0) \geq 0$.
\end{proof}
\begin{corollary}
If $f : \R^n \supseteq \Omega \to \R$ is $C^2$ and $x_0$ is a local minimizer of $f$ in the interior of $\Omega$, then the following conditions hold:
\begin{enumerate}[(i)]
\item $\nabla f(x_0) = 0$.
\item $\nabla^2 f(x_0)$ is positive semidefinite.
\end{enumerate}
\end{corollary}

% check over this
\subsection{Sylvester's Criterion}
Here's a useful criterion for determining when a matrix is positive definite or positive semidefinite.
\begin{definition}
A principal minor of a square matrix $A$ is the determinant of a submatrix of $A$ obtained by removing any $k$ rows and the corresponding $k$ columns, $k \geq 0$. A leading principal minor of $A$ is the determinant of a submatrix obtained by removing the last $k$ rows and $k$ columns of $A$, $k \geq 0$.
\end{definition}

\begin{theorem}
(Sylvester's criterion for positive definite self-adjoint matrices) If $A$ is a self-adjoint matrix, then $A \succ 0$ if and only if all of the leading principal minors of $A$ are positive.
\end{theorem}
\begin{theorem}
(Sylvester's criterion for positive semidefinite self-adjoint matrices) If $A$ is a self-adjoint matrix, then $A \succeq 0$ if and only if all of the principal minors of $A$ are non-negative.
\end{theorem}

\subsection{Examples of using the SONC}

\begin{enumerate}
\item 
Consider the problem
\begin{align*}
\min_{x \in \Omega} f(x,y) = x^2 - xy + y^2 - 3y \qquad \text{over } \Omega = \R^2.
\end{align*}
Recall that $(1,2)$ was the only candidate for a local minimizer of $f$ on $\Omega$. We now check that the SONC holds. Since $(1,2)$ is an interior point of $\Omega$, we must have $\nabla^2 f(1,2) \succeq 0$. We have
\[
\nabla^2 f(1,2) = \begin{pmatrix}
2 & -1 \\ -1 & 2
\end{pmatrix}.
\]
All of the leading principal minors of $\nabla^2 f(1,2)$ are positive, so $(1,2)$ satisfies the SONC by Sylvester's criterion. 

\item 
Consider the problem
\begin{align*}
\min_{x \in \Omega} f(x,y) = x^2 - x + y + xy \qquad \text{over } \Omega = \{(x,y) \in \R^2 : x,y \geq 0\}.
\end{align*}
Recall that $(1/2, 0)$ was the only candidate for a local minizer of $f$. We have
\[
\nabla^2 f(1/2, 0) = \begin{pmatrix}
2 & 1 \\
1 & 0
\end{pmatrix}.
\]
To satisfy the SONC, we must have 
\[
v^T \nabla^2 f(1/2, 0) v \geq 0
\] 
for all feasible directions $v$ at $(1/2, 0)$ such that $\nabla f(1/2, 0) \cdot v = 0$. We have
\[
\nabla f(1/2, 0) = (0, 3/2),
\]
so if $v = (v_1, 0)$, then $v$ is a feasible direction at $(1/2, 0)$ with $\nabla f(1,2, 0) \cdot v = 0$. Then
\[
v^T \nabla^2 f(1/2, 0) v = \begin{pmatrix}
v_1 & 0
\end{pmatrix}\begin{pmatrix}
2 & 1 \\
1 & 0
\end{pmatrix}\begin{pmatrix}
v_1 \\ 0
\end{pmatrix} = \begin{pmatrix}
v_1 & 0
\end{pmatrix} \begin{pmatrix}
2v_1 \\ v_1
\end{pmatrix} = 2v_1^2 \geq 0.
\]
So the SONC is satisfied.
\end{enumerate}

\subsection{Completing the Square}

Let $A$ be a symmetric positive definite $n \times n$ matrix. Our problem is 
\begin{align*}
\min_{x \in \Omega} f(x) = \frac{1}{2} x^T Ax - b \cdot x \qquad \text{over } \Omega = \R^n.
\end{align*} 
The FONC tells us that if $x_0$ is a local minimizer of $f$, then since $x_0$ is an interior point, $\nabla f(x_0) = 0$. We thus have $Ax_0 = b$, so since $A$ is invertible (positive eigenvalues), $x_0 = A^{-1}b$. Therefore $x_0 = A^{-1}b$ is the \emph{unique} candidate for a local minimizer of $f$ on $\Omega$.


The SONC then tells us that $\nabla^2 f(x_0) = A$, so that $\nabla^2 f(x_0) \succ 0$, implying that $x_0   = A^{-1}b$ is a candidate for a local minimizer of $f$ on $\Omega$.

In fact, the candidate $x_0$ is a global minimizer. Why? We will "complete the square". We can write
\[
f(x) = 	\frac{1}{2} x^T Ax - b \cdot x = \frac{1}{2}(x - x_0)^T A(x-x_0) - \frac{1}{2} x_0^T A x_0;
\]
this relies on symmetry. (Long rearranging of terms.) In this form it is obvious that $x_0$ is a global minimizer of $f$ over $\Omega$.

\newpage


\section{Sufficient Condition for an Interior Local Minimizer (May 21)}

\subsection{A Sufficient Condition}
\begin{lemma} 
If $A$ is symmetric and positive-definite, then  there is an $a > 0$ such that $v^T A v \geq a \|v\|^2$ for all $v$.
\end{lemma}
\begin{proof}
There is an orthogonal matrix $Q$ with $Q^T A Q = \mathrm{diag}(\lambda_1, \dots, \lambda_n)$. If $v = Qw$,
\begin{align*}
v^T A v &= (Qw)^T A Qw \\
&= w^T (Q^T A Q) w \\
&= \lambda_1 w_1^2 + \cdots + \lambda_n w_n^2 \\
&\geq \min\{\lambda_1, \dots, \lambda_n\} \|w\|^2 \\
&= \min\{\lambda_1, \dots, \lambda_n\} \|v\|^2 \qquad \text{since $Q$ is orthogonal}
\end{align*}
Since $A$ is positive-definite, every eigenvalue is positive and we are done.
\end{proof}

\begin{theorem}
(Second order sufficient conditions for interior local minimizers) Let $f$ be $C^2$ on $\Omega \subseteq \R^n$, and let $x_0$ be an interior point of $\Omega$ such that $\nabla f(x_0) = 0$ and $\nabla^2 f(x_0) \succ 0$. Then $x_0$ is a strict local minimizer of $f$.
\end{theorem}
\begin{proof}
The condition $\nabla^2 f(x_0) \succ 0$ implies there is an $a > 0$ such that $v^T \nabla^2 f(x_0) v \geq a \cdot \|v\|^2$ for all $v$. By Taylor's theorem we have
\[
f(x_0 + v) - f(x_0) = \frac{1}{2} v^T \nabla^2 f(x_0) v + o(\|v\|^2) \geq \frac{1}{2} a\|v\|^2 + o(\|v\|^2) = \|v\|^2 \left( \frac{a}{2} + \frac{o(\|v\|^2)}{\|v\|^2} \right).
\]
For sufficiently small $v$ the right hand side is positive, so $f(x_0 + v) > f(x_0)$ for all such $v$. Therefore $x_0$ is a strict local minimizer of $f$ on $\Omega$.
\end{proof}

\subsection{Examples}
\begin{enumerate}[(i)]
\item 
Consider $f(x,y) = xy$. The gradient is $\nabla f(x,y) = (y,x)$ and the Hessian is 
\[
\nabla^2 f(x,y) = \begin{pmatrix}
0 & 1 \\ 1 & 0
\end{pmatrix}.
\]
Suppose we want to minimize $f$ on all of $\Omega = \R^2$. By the FONC, the only candidate for a local minimizer is $(0,0)$. The Hessian's eigenvalues are $\pm 1$, so it is not positive definite. We conclude by the SONC that the origin is not a local minimizer of $f$.

\item 
Consider the same function $f(x,y) = xy$ on $\Omega = \{(x,y) \in \R^2, x, y \geq 0\}$. We claim that every point of the boundary of $\Omega$ is a local minimizer of $f$.

Consider $(x,0)$ with $x > 0$. The feasible directions here are $v$ with $v_2 \geq 0$. The FONC tells us that $\nabla f(x,0) \cdot v\geq 0$. This dot product is $xv_2 \geq 0$, so $(x,0)$ satisfies the FONC. Therefore every point on the positive x-axis is a candidate for a local minimizer. As for the SONC, $\nabla f(x,0) \cdot v = xv_2 = 0$ if and only if $v_2 = 0$. Then $v^T \nabla^2 f(x,0) v = 0$. Of course, this tells us nothing; we need a sufficient condition that works for boundary points. That's for next lecture.

Or, you could just say that $f = 0$ on the boundary of $\Omega$ and is positive on the interior, so every point of the boundary of $\Omega$ is a local minimizer (not strict) of $f$.

\end{enumerate}

\newpage
\section{Constrained Optimization (May 26)}

Consider the following minimization problem:
\begin{align*}
\text{minimize } &f(x,y) = xy \\
\text{subject to } &x^2 + y^2 \leq 1
\end{align*}
Let $\Omega$ be the feasible set. The feasible directions at a point $(x_0, y_0) \in \Omega$ are the $(v, w) \in \R^2$ such that $(v, w) \cdot (x_0, y_0) < 0$, or $vx_0 + wy_0 < 0$. By the FONC for a minimizer, $\nabla f(x_0, y_0) \cdot (v, w) \geq 0$, so $wx_0 + vy_0 \geq 0$. Note that a local minimum must occur on the boundary. (Why?) We have three cases, depending on the sign of $x_0 + y_0$.
\begin{enumerate}[(i)]
\item $x_0 + y_0 < 0$: can't occur
\item $x_0 + y_0 > 0$: can't occur
\item $x_0 + y_0 = 0$: good!
\end{enumerate}
(This part could not be finished as attention had to be diverted from the lecture.)

\subsection{Second Order Necessary Condition for a Local Minimizer}

\begin{theorem}
(Second order sufficient condition for a local minimizer) Let $f$ be $C^2$ on $\Omega \subseteq \R^n$ and suppose $x_0 \in \Omega$ satisfies
\begin{enumerate}[(i)]
\item $\nabla f(x_0) \cdot v \geq 0$ for all feasible directions $v$ at $x_0$,
\item if $\nabla f(x_0) \cdot v = 0$ for some such $v$, then $v^T \nabla^2 f(x_0) v > 0$.
\end{enumerate}
Then $x_0$ is a local minimizer of $f$ on $\Omega$.
\end{theorem}

\subsection{Optimization with Equality Constraints}

Consider the minimization problem
\begin{align*}
\text{minimize } &f(x,y) \\
\text{subject to } &h(x,y) = x^2 + y^2 - 1 = 0
\end{align*}
Suppose $(x_0, y_0)$ is a local minimizer. Two cases:
\begin{enumerate}
\item $\nabla f(x_0, y_0) \neq 0$: we claim that $\nabla f(x_0, y_0)$ is perpendicular to the tangent space to the unit circle $h^{-1}(\{0\})$ at $(x_0, y_0)$. If this is not the case, then we obtain a contradiction by looking at the level sets of $f$, to which $\nabla f$ is perpendicular. Therefore $\nabla f(x_0, y_0) = \lambda \nabla h(x_0, y_0)$ for some $\lambda$.

\item $\nabla f(x_0, y_0) = 0$: as in the previous case, $\lambda = 0$.
\end{enumerate}
In either case, at a local minimizer, the gradient of the function to be minimized is parallel to the gradient of the constraints.

We now recall some elementary differential geometry.
\begin{definition}
For us, a surface is the set of common zeroes of a finite set of $C^1$ functions. 
\end{definition}
\begin{definition}
For us, a differentiable curve on the surface $M \subseteq \R^n$ is the image of a $C^1$ function $x : (a, b) \to M$.
\end{definition}
\begin{definition}
Let $x(s)$ be a differentiable curve on $M$ that passes through $x_0 \in M$ at time $x(0) = x_0$. The velocity vector $v = \left. \frac{d}{ds} \right|_{s=0} x(s)$ of $x(s)$ at $x_0$ is, for us, said to be a tangent vector to the surface $M$ at $x_0$. The set of all tangent vectors to $M$ at $x_0$ is called the tangent space to $M$ at $x_0$ and is denoted by $T_{x_0}M$.
\end{definition}
\begin{definition}
Let $M = \{x \in \R^n : h_1(x) = \cdots = h_k(x) = 0\}$ be a surface. If $\nabla h_1(x_0), \dots, \nabla h_k(x_0)$ are all linearly independent, then $x_0$ is said to be a regular point of $M$.
\end{definition}
\begin{theorem}
At a regular point $x_0 \in M$, the tangent space $T_{x_0} M$ is given by
\[
T_{x_0} M = \{ y \in \R^n : \nabla \mathbf{h}(x_0)y = 0 \}.
\]
\end{theorem}
\begin{proof}
It's in the book. Use the implicit function theorem.
\end{proof}
\begin{lemma}
Let $f, h_1, \dots, h_k$ be $C^1$ functions on the open set $\Omega \subseteq \R^n$. Let $x_0 \in M = \{ x \in \Omega : h_1(x) = \cdots = h_k(x) = 0 \}$. Suppose $x_0$ is a local minimizer of $f$ subject to the constraints $h_i(x) = 0$. Then $\nabla f(x_0)$ is perpendicular to $T_{x_0}M$.
\end{lemma}
\begin{proof}
Without loss of generality, suppose $\Omega = \R^n$. Let $v \in T_{x_0}M$. Then $v = \left. \frac{d}{ds} \right|_{s=0}x(s)$ for some differentiable curve $x(s)$ in $M$ with $x(0) = x_0$. Since $x_0$ is a local minimizer of $f$, $0$ is a local minimizer of $f \circ x$, so $\nabla f(x_0) \cdot x'(0) = \nabla f(x_0) \cdot v = 0$.
\end{proof}

\newpage

\section{Lagrange Multipliers (May 28)}

\subsection{First Order Necessary Condition for a Local Minimizer Under Equality Constraints}

Here is the first order necessary condition for a local minimizer under equality constraints.
\begin{theorem}
(Lagrange multipliers) Let $f, h_1, \dots, h_k$ be $C^1$ functions on some open $\Omega \subseteq \R^n$. Suppose $x_0$ is a local minimizer of $f$ subject to the constraints $h_1(x), \dots, h_k(x) = 0$, which is also a regular point of these constraints. Then there are $\lambda_1, \dots, \lambda_k \in \R$ ("Lagrange multipliers") such that
\[
\nabla f(x_0) + \lambda_1 \nabla h_1(x_0) + \cdots + \lambda_k \nabla h_k(x_0) = 0.
\]
\end{theorem}
\begin{proof}
Since $x_0$ is regular, $T_{x_0}M = \mathrm{span}(\{ \nabla h_1(x_0), \dots, \nabla h_k(x_0) \})^\perp$. By a lemma from last class, $\nabla f(x_0) \in (T_{x_0}M)^\perp$. Therefore $\nabla f(x_0) \in \mathrm{span}(\{ \nabla h_1(x_0), \dots, \nabla h_k(x_0) \})$, since we are dealing with a finite dimensional vector space. We are done.
\end{proof}

\subsection{The Box Example}

Given a fixed area $A > 0$, how do we construct a box of maximum volume with surface area $A$? Suppose the volume is $V(x,y,z) = xyz$ and the area is $A(x,y,z) = 2(xy+xz+yz)$. Our problem is stated as a maximization problem, so we have to convert it to a minimization problem. Let $f = -V$. We are therefore dealing with the problem
\begin{align*}
\text{minimize } &f(x,y,z) = -xyz \\
\text{subject to } &h(x,y,z) = A(x,y,z) - A = 0, x,y,z \geq 0
\end{align*}
But we don't know how to deal with inequality constraints right now, so we have to make some changes. Note that if any one of $x,y,z$ is zero, then the volume is zero. Therefore the problem we want to consider is really the problem
\begin{align*}
\text{minimize } &f(x,y,z) \\
\text{subject to } &h(x,y,z) = 0, x,y,z > 0
\end{align*}
Now, if $\Omega = \{(x,y,z) \in \R^3 : x,y,z > 0\}$, then the above minimization problem may be solved using the first order necessary condition we gave above, for the set $\Omega$ is open.

Suppose $(x_0, y_0, z_0)$ is a local minimizer of $f$ subject to the constraint $h(x,y,z) = 0$. This point is regular because we are only considering points whose coordinates are all positive. Then there is a $\lambda \in \R$ such that $\nabla f(x_0, y_0, z_0) + \lambda \nabla h(x_0, y_0, z_0) = 0$. Therefore
\[
(-y_0z_0, -x_0z_0, -x_0y_0) + \lambda (2y_0 + 2z_0, 2x_0 + 2z_0, 2x_0 + 2y_0) = (0,0,0).
\]
Equivalently, 
\begin{align*}
2\lambda (y_0 + z_0) &= y_0z_0 \\
2\lambda (x_0 + z_0) &= x_0z_0 \\
2\lambda (x_0 + y_0) &= x_0y_0
\end{align*}
Add all of these equations together:
\[
2\lambda( 2x_0 + 2y_0 + 2z_0 ) = x_0z_0 + x_0y_0 + y_0z_0 = \frac{A}{2} > 0
\]
implying that $\lambda > 0$. The first two equations tell us that
\begin{align*}
2\lambda x_0 (y_0 + z_0) &= x_0y_0z_0 \\
2\lambda y_0 (x_0 + z_0) &= x_0y_0z_0.
\end{align*}
Subtracting these two equations gives $2\lambda (x_0z_0 - y_0z_0) = 0$. Cancelling the $z_0$'s gives $2\lambda (x_0 - y_0) = 0$, and since $\lambda > 0$, we have $x_0 = y_0$. Since we could have done the same thing with the other pairs of equations, we get $x_0 = y_0 = z_0$. 

Physically, this tells us that in order to maximize the volume of a rectangular solid of fixed area, we must make a cube. Note that we haven't actually solved the maximization problem; we've only figured out what form its solutions must take.

\subsection{Second Order Necessary Conditions for a Local Minimizer Under Equality Constraints}

\begin{theorem}
Let $f, h_1, \dots, h_k$ be $C^2$ on some open set $\Omega \subseteq \R^n$. Suppose $x_0$ is a regular point which is a local minimizer of $f$ subject to the constraints. Then
\begin{enumerate}[(i)]
\item
There are $\lambda_1, \dots, \lambda_k \in \R$ such that
\[
\nabla f(x_0) + \lambda_1 \nabla h_1(x_0) + \cdots + \lambda_k \nabla h_k(x_0) = 0.
\] 

\item
The "Lagrangian"
\[
L(x_0) = \nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h_i(x_0)
\]
is positive semi-definite on the tangent space $T_{x_0}M$, where $M = h_1^{-1}(\{0\}) \cap \cdots \cap h_k^{-1}(\{0\})$.
\end{enumerate}
\end{theorem}

\newpage

\section{More on Optimization with Equality Constraints (June 2)}

\subsection{SONC and SOSC, Equality Constraints}

\begin{theorem}
(Second order necessary conditions for a local minimizer with equality constraints) Consider functions $f, h_1, \dots, h_k$ which are $C^2$ on the open $\Omega \subseteq \R^n$. Suppose $x_0$ is a regular point of the constraints given by $h_1(x) = \cdots = h_k(x) = 0$, and that it is a local minimizer of $f$ on $M = \bigcap h_i^{-1}(\{0\})$. Then
\begin{enumerate}
\item
There exist $\lambda_1, \dots, \lambda_k \in \R$ such that
\[
\nabla f(x_0) + \sum \lambda_i \nabla h_i(x_0) = 0.
\]

\item
The Lagrangian
\[
L(x_0) = \nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h_i(x_0)
\]
is positive semi-definite on $T_{x_0}M$.
\end{enumerate}
\end{theorem}

\begin{proof}
Let $x(s)$ be a smooth curve with $x(0) = 0$ in $M$. Recall that, by the product rule,
\begin{align*}
\frac{d}{ds} f(x(s)) &= \nabla f(x(s)) \cdot x'(s) \\
\frac{d^2}{ds^2} f(x(s)) &= x'(s) \cdot \nabla^2 f(x(s)) x'(s) + \nabla f(x(s)) \cdot x''(s).
\end{align*}
By the second order Taylor approximation, we have
\[
0 \leq f(x(s)) - f(x(0)) = s \left. \frac{d}{ds} \right|_{s=0} f(x(s)) + \frac{1}{2}s^2 \left. \frac{d^2}{ds^2} \right|_{s=0} f(x(s)) + o(s^2).
\]
This is, equivalently, 
\[
0 \leq f(x(s)) - f(x(0)) = s \nabla f(x_0) \cdot \underbrace{x'(0)}_{\in T_{x_0}M} + \frac{1}{2}s^2 \left. \frac{d^2}{ds^2} \right|_{s=0} f(x(s)) + o(s^2).
\]
Since the gradient at a regular local minimizer is perpendicular to the tangent space there, the first-order term above vanishes. We have
\[
0 \leq \frac{1}{2}s^2 \left. \frac{d^2}{ds^2} \right|_{s=0} f(x(s)) + o(s^2).
\]
By the definition of $M$, we may write the above as
\[
0 \leq \frac{1}{2}s^2 \left. \frac{d^2}{ds^2} \right|_{s=0} \left[ f(x(s)) + \sum \lambda_i h_i(x(s)) \right] + o(s^2).
\]
Or
\[
0 \leq \frac{1}{2} s^2 x'(0) \cdot \underbrace{\left(\nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h(x_0)\right)}_{=L(x_0)}x'(0) + \frac{1}{2} s^2 \underbrace{\left(\nabla f(x_0) + \sum \lambda_i \nabla h_i(x_0)\right)}_{=0} \cdot x''(0) + o(s^2).
\]
Divide by $s^2$:
\[
0 \leq \frac{1}{2} x'(0) \cdot L(x_0) x'(0) + \frac{o(s^2)}{s^2}.
\]
By taking $s$ small it follows that $0 \leq \frac{1}{2} x'(0) \cdot L(x_0) x'(0)$. Since any tangent vector $v \in T_{x_0}M$ can be described as the tangent vector to a curve in $M$ through $x_0$, it follows that $L(x_0)$ is positive semi-definite on $T_{x_0}M$.
\end{proof}

\begin{theorem}
(Second order sufficient conditions for a local minimizer with equality constraints) Consider functions $f, h_1, \dots, h_k$ which are $C^2$ on the open $\Omega \subseteq \R^n$. Suppose $x_0$ is a regular point of the constraints given by $h_1(x) = \cdots = h_k(x) = 0$. Let $M = \bigcap h_i^{-1}(\{0\})$. Suppose there exist $\lambda_1, \dots, \lambda_k \in \R$ such that
\begin{enumerate}
\item
\[
\nabla f(x_0) + \sum \lambda_i \nabla h_i(x_0) = 0
\]

\item
The Lagrangian
\[
L(x_0) = \nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h_i(x_0)
\]
is positive definite on $T_{x_0}M$.
\end{enumerate}
Then $x_0$ is a strict local minimizer of $f$ on $M$.
\end{theorem}
\begin{proof}
Recall that if $L(x_0)$ is positive definite on $T_{x_0}M$, then there is an $a > 0$ such that $v \cdot L(x_0)v \geq a\|v\|^2$ for all $v \in T_{x_0}M$. (This is very easily proven by diagonalizing the matrix.) Let $x(s)$ be a smooth curve in $M$ such that $x(0) = x_0$, and normalize the curve so that $\|x'(0)\| = 1$. We have
which becomes
\begin{align*}
f(x(s)) - f(x(0)) &= s \left. \frac{d}{ds} \right|_{s=0} f(x(s)) + \frac{1}{2} s^2 \left. \frac{d^2}{ds^2} \right|_{s=0} f(x(s)) + o(s^2) \\
&= s \left. \frac{d}{ds} \right|_{s=0} \left[ f(x(s)) + \sum \lambda_i h_i(x(s)) \right]+ \frac{1}{2} s^2 \left. \frac{d^2}{ds^2} \right|_{s=0} \left[ f(x(s)) + \sum \lambda_i h_i(x(s)) \right] + o(s^2) \\
&= s \underbrace{[ \nabla f(x_0) + \sum \lambda_i \nabla h_i(x_0) ]}_{=0 \text{ by 1.}} \cdot x'(0) + \frac{1}{2} s^2x'(0) \cdot L(x_0) x'(0)  \\
&\qquad\qquad\qquad\qquad\qquad + \frac{1}{2} s^2\underbrace{[ \nabla f(x_0) + \sum \lambda_i \nabla h_i(x_0) ]}_{=0 \text{ by 1.}} \cdot x''(0) + o(s^2) \\
&= \frac{1}{2} s^2 x'(0)^T L(x_0) x'(0) + o(s^2) \\
&\geq \frac{1}{2}s^2 a\|x'(0)\|^2 + o(s^2) \\
&= \frac{1}{2}s^2 a + o(s^2) \\
&= s^2 \left( \frac{1}{2}a + \frac{o(s^2)}{s^2} \right)
\end{align*}
For sufficiently small $s$, the above is positive, so $f(x(s)) > f(x_0)$ for all sufficiently small $s$. Since $x(s)$ was arbitrary, $x_0$ is a strict local minimizer of $f$ on $M$.
\end{proof}

\subsection{Examples}

\begin{enumerate}
\item
Recall the box example: maximizing the volume of a box of sides $x,y,z\geq 0$ subject to a fixed surface area $A > 0$. We were really minimizing the negative of the volume. We got $(x_0,y_0,z_0) = (l,l,l)$, where $l = \sqrt{A/6}$. Our Lagrange multiplier was $\lambda = \frac{A}{8(x_0+y_0+z_0)} = \frac{A}{24 l} > 0$. We had (after some calculation)
\[
L(x_0,y_0,z_0) = (2\lambda - l)\begin{pmatrix}
0&1&1 \\
1&0&1 \\
1&1&0
\end{pmatrix}.
\]
Here, $2\lambda - l < 0$. We have 
\[
T_{(x_0,y_0,z_0)}M = \mathrm{span}( \nabla h(x_0,y_0,z_0) )^\perp = \{ (u,v,w) \in \R^3 : u+v+w=0 \},
\]
since $\nabla h(x_0,y_0,z_0) = (4l,4l,4l)$. If $(u,v,w) \in T_{(x_0,y_0,z_0)}M$ is nonzero,
\begin{align*}
\begin{pmatrix}
u&v&w
\end{pmatrix}(2\lambda - l)\begin{pmatrix}
0&1&1 \\
1&0&1 \\
1&1&0
\end{pmatrix} \begin{pmatrix}
u\\v\\w
\end{pmatrix} &= \begin{pmatrix}
u&v&w
\end{pmatrix}(2\lambda - l) \begin{pmatrix}
v+w \\ u+w \\ u+v
\end{pmatrix} \\
&= (2\lambda - l)\begin{pmatrix}
u&v&w
\end{pmatrix} \begin{pmatrix}
-u \\ -v \\ -w
\end{pmatrix} \\
&= -(2\lambda - l)(u^2+v^2+w^2) > 0,
\end{align*}
so by the SOSC under equality constraints, our point $(x_0,y_0,z_0)$ is a strict local maximizer of the volume. In fact, it is a strict global minimum (which is yet to be seen).

\item
Consider the problem
\begin{align*}
\text{minimize } &f(x,y) = x^2-y^2 \\
\text{subject to } &h(x,y) = y = 0.
\end{align*}
Then
\[
\nabla f(x,y) + \lambda \nabla h(x,y) = \begin{pmatrix}
2x \\ -2y
\end{pmatrix} + \lambda \begin{pmatrix}
0 \\ 1
\end{pmatrix} = \begin{pmatrix}
0 \\ 0
\end{pmatrix},
\]
implying that $\lambda = 0$ and that $(x,y) = (0,0)$ is our candidate local minimizer. Since $\nabla h(x,y) \neq (0,0)$, the candidate is a regular point. We have
\[
L(0,0) = \begin{pmatrix}
2 & 0 \\ 0 & -2
\end{pmatrix} + 0 \begin{pmatrix}
0 & 0 \\ 0 & 0
\end{pmatrix} = \begin{pmatrix}
2 & 0 \\ 0 & -2
\end{pmatrix},
\]
which is not positive semi-definite \emph{everywhere}. What about on the tangent space $T_{(0,0)}(\text{$x$-axis})=(\text{$x$-axis})$? Clearly it is positive definite on the $x$-axis, so by the SOSC that we just proved, $(0,0)$ is a strict local minimizer of $f$ on the $x$-axis. Thinking of level sets, this is intuitively true.

\item
Consider the problem
\begin{align*}
\text{minimize } &f(x,y) = (x-a)^2 + (y-b)^2 \\
\text{subject to } &h(x,y) = x^2+y^2-1=0.
\end{align*}
Let us assume that $(a,b)$ satisfies $a^2+b^2>1$. We have $\nabla h(x,y) = (2x,2y)$, which is non-zero on $S^1$, implying that every point of $S^1$ is a regular point. Lagrange tells us that
\[
\begin{pmatrix}
2(x-a) \\ 2(y-b)
\end{pmatrix} + \lambda
\begin{pmatrix}
2x \\ 2y
\end{pmatrix} = \begin{pmatrix}
0\\0
\end{pmatrix},
\]
as well as $x^2+y^2=1$. This may be written
\begin{align*}
(1+\lambda)x &= a \\
(1+\lambda)y &= b \\
x^2+y^2 &= 1
\end{align*}
By our assumption that $a^2+b^2>1$, we have $\lambda \neq -1$. Therefore
\[
\begin{pmatrix}
x \\ y
\end{pmatrix} = \frac{1}{1+\lambda}\begin{pmatrix}
a \\ b
\end{pmatrix},
\]
which implies that
\[
\frac{1}{1+\lambda} = \frac{1}{\sqrt{a^2+b^2}}
\]
by the third equation. Therefore
\[
\begin{pmatrix}
x_0 \\ y_0
\end{pmatrix} = \frac{1}{\sqrt{a^2+b^2}}\begin{pmatrix}
a \\ b
\end{pmatrix}.
\]
Thinking of level sets, this is intuitively true. The Lagrangian is
\[
L(x_0,y_0) = \begin{pmatrix}
2 & 0 \\ 0 & 2
\end{pmatrix} + \lambda \begin{pmatrix}
2 & 0 \\ 0 & 2
\end{pmatrix} = \underbrace{(1+\lambda)}_{>0}\begin{pmatrix}
2 & 0 \\ 0 & 2
\end{pmatrix},
\]
which, by the SOSC that we proved, proves that $(x_0, y_0)$ is a strict local minimizer of $f$ on $S^1$. In fact, this point is a global minimizer of $f$ on $S^1$, which follows immediately by the fact that $f$ necessarily takes on a global minimum on $S^1$ and that it only takes on the point $(x_0,y_0)$.

\item
For a special case, we will derive the Lagrange multipliers equation. Suppose we are working with $C^1$ functions $f,h$. Our problem is
\begin{align*}
\text{minimize } &f(x,y,z) \\
\text{subject to } &g(x,y,z) = z-h(x,y) = 0.
\end{align*}
That is, we are minimizing $f(x,y,z)$ on the graph $\Gamma_h$ of $h$. The Lagrange equation tells us that
\[
\nabla f(x,y,z) + \lambda g(x,y,z) = \begin{pmatrix}
\frac{\pd f}{\pd x}(x,y,z) \\ \frac{\pd f}{\pd y}(x,y,z) \\ \frac{\pd f}{\pd z}(x,y,z)
\end{pmatrix} + \lambda \begin{pmatrix}
-\frac{\pd h}{\pd x}(x,y,z) \\ -\frac{\pd y}{\pd x}(x,y,z) \\ 1
\end{pmatrix} = \begin{pmatrix}
0 \\ 0 \\ 0
\end{pmatrix}.
\]

We will derive the above formula by expressing it as an unconstrained minimization problem
\[ 
\text{minimize }_{(x,y) \in \R^2} F(x,y)
\]
for some function $F$. We will then find the first order necessary conditions for an unconstrained minimization, and then express it as the equation we would like to prove.

Define $F(x,y) = f(x,y,f(x,y))$. The constrained minimization problem is therefore equivalent to the unconstrained problem. By our theory of unconstrained minimization, $\nabla F(x_0,y_0)=(0,0)$. That is,
\[
\nabla F(x_0,y_0) = \begin{pmatrix}
\frac{\pd f}{\pd x} + \frac{\pd f}{\pd z}\frac{\pd h}{\pd x} \\
\frac{\pd f}{\pd y} + \frac{\pd f}{\pd z}\frac{\pd h}{\pd y}
\end{pmatrix} = \begin{pmatrix}
0\\0
\end{pmatrix}.
\]
Rather,
\begin{align*}
\frac{\pd f}{\pd x} + \frac{\pd f}{\pd z}\frac{\pd h}{\pd x} &= 0 \\
\frac{\pd f}{\pd y} + \frac{\pd f}{\pd z}\frac{\pd h}{\pd y} &= 0
\end{align*}
Let $\lambda = -\frac{\pd f}{\pd z}$. The equation becomes
\begin{align*}
\frac{\pd f}{\pd x} - \lambda \frac{\pd h}{\pd x} &= 0 \\
\frac{\pd f}{\pd y} - \lambda \frac{\pd h}{\pd y} &= 0 \\
\frac{\pd f}{\pd z} + \lambda &= 0
\end{align*}
which is what we wanted.
\end{enumerate}

\newpage

\section{Optimization under Inequality Constraints (June 4)}

\subsection{Kuhn-Tucker Conditions}

Our problem is of the form
\begin{align*}
\text{minimize } &f(x) \\
\text{subject to } &h_1(x) = \cdots = h_k(x) = 0 \\
&g_1(x), \dots, g_l(x) \leq 0.
\end{align*}
\begin{definition}
Let $x_0$ satisfy the above constraints. We call the inequality constraint $g_i(x) \leq 0$ active at $x_0$ if $g_i(x_0) = 0$. Otherwise, it is inactive at $x_0$.
\end{definition}
Since we are only studying local properties of functions, we will only be concerned with active constraints.

\begin{definition}
Suppose there is an index $l' \leq l$ such that $g_1(x_0) =, \dots, g_{l'}(x_0) = 0$ are active, and $g_{l'+1}(x_0) \leq 0, \dots, g_l(x_0) \leq 0$ are inactive. We say that $x_0$ is a regular point of these constraints if the vectors $\nabla h_1(x_0), \dots, \nabla h_k(x_0), \nabla g_1(x_0), \dots, \nabla g_{l'}(x_0)$ are linearly independent.
\end{definition}
\begin{theorem}
(First order necessary conditions for minimizers under inequality constraints) Let $\Omega \subseteq \R^n$ be open and consider $C^1$ functions $f, h_1, \dots, h_k, g_1, \dots, g_l$ on $\Omega$. Suppose $x_0$ is a local minimizer of $f$ subject to the constraints, and that $x_0$ is regular as defined above. Then 
\begin{enumerate}[(i)]
\item There exist $\lambda_1, \dots, \lambda_k \in \R$ and $\mu_1, \dots, \mu_l \in \R^{\geq 0}$ such that 
\[
\nabla f(x_0) + \sum \lambda_i \nabla h_i(x_0) + \sum \mu_j \nabla g_j(x_0) = 0.
\]
\item ("Complementary slackness conditions") For all $j$, $\mu_j g_j(x_0) = 0$, or equivalently, $\sum \mu_j g_j(x_0) = 0$.
\end{enumerate}
\end{theorem}
These conditions are also known as the \emph{Kuhn-Tucker conditions}.

Suppose the active constraints at $x_0$ are the first $l'$ constraints. Since each $\mu_j \geq 0$, condition (ii) is equivalent to saying that if $j \geq l'+1$, then $\mu_j = 0$. 

\begin{proof}
If $x_0$ is a local minimizer of $f$ subject to the constraints, then it is certainly a local minimizer of $f$ subject to only the active constraints. That is, $x_0$ is also a local minimizer of $f$ subject to the equality constraints
\[
h_1(x) = \cdots = h_k(x) = g_1(x) = \cdots = g_{l'} = 0.
\]
We know how to work with this! Let $M$ be the surface defined by these equality constraints. By the Lagrange multipliers theorem, 
\[
\nabla f(x_0) + \sum \lambda_i \nabla h_i(x_0) + \sum \mu_j \nabla g_j(x_0) = 0,
\]
for some $\lambda_i \in \R, \mu_j \in \R$. (Note that we have not yet shown that the $\mu_j$'s are non-negative.)

Note that $g_1(x_0) = \cdots = g_{l'}(x_0) = 0$. Therefore $\mu_{l'+1} = \cdots = \mu_l = 0$, so it follows that
\[
\mu_1 g_1(x_0) = 0, \dots, \mu_{l'}g_{l'}(x_0) = 0,
\]
which implies that $\mu_j g_j(x_0) = 0$ for all $j$. We have proven condition (ii).

We must now verify the non-negativity of the $\mu_j$'s. Suppose for the sake of contradiction that some $\mu_j < 0$; WLOG assume $j=1$. Let
\[
\tilde{M} = \{x \in \Omega : h_i(x) = 0, g_i(x) = 0, j \neq 1\}.
\]
Since $x_0$ is a regular point of $M$, $x_0$ is a regular point of $\tilde{M}$. Therefore
\[
T_{x_0}\tilde{M} = \mathrm{span}(\{\nabla h_1(x_0), \dots, h_k(x_0), \nabla g_2(x_0), \dots, \nabla g_l(x_0)\})^\perp.
\]
The vector $\nabla g_1(x_0)$ does not lie in this span, so there is a $v \in T_{x_0}\tilde{M}$ such that $\nabla g_1(x_0) \cdot v < 0$. That is, $g_1$ is strictly decreasing in the direction of $v$, or in more precise language, $g_1(x_0 + sv) < g_1(x_0)$ for all sufficiently small $s$, as we have
\[
\left. \frac{d}{ds} \right|_{s=0} g_1(x_0 + sv) = \nabla g_1(x_0) \cdot v < 0. 
\]
Therefore $v$ is a feasible direction for $g_1(x) \leq 0$ at $x_0$, and also, $v$ is tangential to the other constraints. Since $x_0$ is a regular point of $\tilde{M}$, we may find a curve $x(s)$ on $\tilde{M}$ such that $x(0) = x_0$ and $x'(0) = v$. Also, $s = 0$ is a local minimizer of $f \circ x$, so
\[
\left. \frac{d}{ds}\right|_{s=0} f(x(s)) = \nabla f(x_0) \cdot v \geq 0.
\]
On the other hand, 
\[
\nabla f(x_0) + \sum \lambda_i \nabla h_i(x_0) + \mu_1 \nabla g_1(x_0)+\sum_{j=2}^{l'} \mu_j \nabla g_j(x_0) = 0.
\]
Taking the dot product of the above equation by $v$ kills the two sums above and gives
\[
\nabla f(x_0)\cdot v  + \mu_1 \nabla g_1(x_0)\cdot v = 0,
\]
implying $\nabla f(x_0)\cdot v < 0$, a contradiction. So every $\mu_j \geq 0$.
\end{proof}

\newpage

\section{More on Inequality Constraints (June 9)}

As before, we are working on an open $\Omega \subseteq \R^n$, and we want to optimize $f$ subject to $h_1, \dots, h_k = 0$ and $g_1, \dots, g_l \leq 0$. The smoothness of our functions varies.

\subsection{Second Order Conditions}

One might guess that the second order conditions under inequality constraints will be the same thing as before. However, the tangent space on which we evaluate the positive-definiteness of the Lagrangian is slightly different (in a very obvious way).

\begin{theorem}
Suppose $f, h_1, \dots, h_k, g_1, \dots, g_k \in C^2(\Omega)$, where $\Omega \subseteq \R^n$. Suppose $x_0$ is a regular point of the constraints. If $x_0$ is a local minimizer of $f$ subject to the constraints, then
\begin{enumerate}[(i)]
\item
There are $\lambda_1, \dots, \lambda_k \in \R$ and $\mu_1, \dots, \mu_l \geq 0$ such that
\[
\nabla f(x_0) + \sum_i \lambda_i \nabla h_i(x_0) + \sum_j \mu_j \nabla g_j(x_0) = 0,
\]
and $\mu_j g_j(x_0) = 0$ for each $j$.

\item
The matrix
\[
L(x_0) = \nabla^2 f(x_0) + \sum_i \lambda_i \nabla^2 h_i(x_0) + \sum_j \mu_j \nabla^2 g_j(x_0)
\]
is positive semi-definite on the tangent space $T_{x_0}\tilde{M}$ to the active constraints at $x_0$. (Explicitly, $L(x_0)$ is positive semi-definite on the space
\[
T_{x_0}\tilde{M} = \{v \in \R^n : \nabla h_i(x_0) \cdot v = 0 \text{ for all $i$, and } \nabla g_j(x_0) \cdot v = 0 \text{ for all $1 \leq j \leq l'$}\},
\]
where the active $g$ constraints are indexed precisely by $1, \dots, l'$.)
\end{enumerate}
\end{theorem}
\begin{proof}
$x_0$ is a local minimizer of $f$ subject to the constraints, so it is also a local minimizer of $f$ subject to only the active constraints. Since the Lagrange multiplies of the inactive constraints are zero, our theory of equality-constrained minimization finishes the problem.
\end{proof}

\begin{enumerate}
\item
Consider, for example, the problem
\begin{align*}
\text{minimize } &f(x,y) := -x \\
\text{subject to } &g_1(x,y) := x^2+y^2 \leq 1 \\
&g_2(x,y) := y+x-1 \leq 0
\end{align*}
The feasible set is the closed unit ball $\overline{B_1(0)}$ with an open semicircle removed from the top right. Geometrically, it is clear that the minimizer should be the point $(1,0)$. It is not hard to check that every feasible point is regular. Let's check that $(x_0, y_0) = (1,0)$ satisfies the first order conditions. We look at
\[
\nabla f (x_0, y_0) + \mu_1 \nabla g_1 (x_0, y_0) + \mu_2 \nabla g_2 (x_0, y_0) = (0,0).
\]
This becomes
\[
\begin{pmatrix}
-1 \\ 0
\end{pmatrix} + \mu_1 \begin{pmatrix}
2 \\ 0
\end{pmatrix} + \mu_2 \begin{pmatrix}
1 \\ 1
\end{pmatrix} = \begin{pmatrix}
0 \\ 0
\end{pmatrix}
\]
or
\begin{align*}
2\mu_1 + \mu_2 &= 1 \\
\mu_2 &= 0.
\end{align*}
So $\mu_1 = 1/2$. Also, $g_1(1,0) = 1^2 + 0^2 - 1 = 0$ and $g_(1,0) = 0$ as well, so the complementary slackness conditions are satisfied. Therefore $(1,0)$ satisfies the Kuhn-Tucker conditions, and so it is a candidate local minimizer. What about the second order conditions?
\[
L(1,0) = \nabla^2 f (x_0, y_0) + \mu_1 \nabla^2 g_1 (x_0, y_0) + \mu_2 \nabla^2 g_2 (x_0, y_0),
\]
or
\[
L(1,0) = I.
\]
Clearly the second order necessary conditions are satisfied, but let's check the tangent space anyway. We have $\nabla g_1(1,0) = (2, 0)$ and $\nabla g_2(1,0) = (1,1)$; they are linearly independent, so the tangent space is a point. Therefore the second order necessary conditions are satisfied.

\item
Consider the problem
\begin{align*}
\text{minimize } &f(x,y) := 2x^2 + 2xy + y^2 - 10x - 10y \\
\text{subject to } &g_1(x,y) = x^2+y^2-5 \leq 0 \\
&g_2(x,y) := 3x+y-6 \leq 0
\end{align*}
The Kuhn-Tucker conditions are
\[
\begin{pmatrix}
4x+2y-10 \\ 2x+2y-10
\end{pmatrix} + \mu_1 \begin{pmatrix}
2x \\ 2y
\end{pmatrix} + \mu_2 \begin{pmatrix}
3 \\ 1
\end{pmatrix} = \begin{pmatrix}
0 \\ 0
\end{pmatrix}
\]
as well as $\mu_1, \mu_2 \geq 0$ and $\mu_1 (x^2+y^2-5) = 0$ and $\mu_2 (3x+y - 6) = 0$. We consider four cases:
\begin{enumerate}[(i)]
\item
Suppose $g_1$ is inactive and $g_2$ is active. Then $\mu_1 = 0$. The equations become
\begin{align*}
4x + 2y - 10  + 3\mu_2 &= 0 \\
2x + 2y - 10 + \mu_2 &= 0 \\
\mu_2 (3x+y-6) &= 0
\end{align*}
Subtracting the second equation from the first gives $x =- \mu_2$. If $\mu_2 = 0$, then $x = 0$, which implies that $y = 6$ since the second constraint is active. We get the point $(0,6)$; but this does not satisfy the constraints. Therefore $\mu_2 \neq 0$. (After some work one can conclude that no such point here satisfies the constraints.) 

\item
Suppose $g_1$ is active and $g_2$ is inactive. Then
\begin{align*}
4x + 2y - 10 + 2\mu_1 x &= 0 \\
2x + 2y - 10 + 2\mu_1 y &= 0 \\
\mu_1(x^2+y^2-5) &= 0 \\
\mu_1 &\geq 0
\end{align*}
The solution is $(1,2)$ and $\mu_1 = 1$. It is not hard to see that this point is regular. Therefore the point $(1,2)$ is a candidate. The Lagrangian is, after some work, 
\[
L(1,2) = \begin{pmatrix}
6 & 2 \\ 2 & 4
\end{pmatrix}.
\]
This matrix is clearly positive definite, so we conclude that the second order necessary (and, as we'll see later, sufficient) conditions are satisfied.

\item
Suppose $g_1$ and $g_2$ are active.

\item
And so on. (This problem was not completed during lecture.)
\end{enumerate}
\end{enumerate}

\subsection{Second Order Sufficient Conditions}

\begin{theorem}
Suppose $f, h_1, \dots, h_k, g_1, \dots, g_k \in C^2(\Omega)$, where $\Omega \subseteq \R^n$ is open. Suppose that $x_0$ is feasible. If
\begin{enumerate}
\item
There exist $\lambda_1, \dots, \lambda_k \in \R$ and $\mu_1, \dots, \mu_l \geq 0$ such that
\[
\nabla f(x_0) + \sum_i \lambda_i \nabla h_i(x_0) + \sum_j \mu_j \nabla g_j(x_0) = 0,
\]

\item
$\mu_j g_j(x_0) = 0$ for each $j$.

\item
The matrix
\[
L(x_0) = \nabla^2 f(x_0) + \sum_i \lambda_i \nabla^2 h_i(x_0) + \sum_j \mu_j \nabla^2 g_j(x_0)
\]
is positive definite on the tangent space to the "strongly active constraints" at $x_0$. That is, it is positive definite on the space
\[
\tilde{\tilde{T_{x_0}}} = \{ v \in \R^n : \nabla h_i(x_0) = 0 \text{ for all $i$, and } \nabla g_j(x_0) = 0 \text{ for all $1 \leq k \leq l''$} \},
\]
where $\{1, \dots, l''\}$ is the set of all indices of active constraints whose Lagrange multipliers are positive.
\end{enumerate}
Then $x_0$ is a strict local minimizer of $f$ subject to the usual constraints.
\end{theorem}
\begin{proof}
Will be given on Thursday. (Copypaste it here?)
\end{proof}

Let's consider some more examples.

\begin{enumerate}
\item
Here's an example. Given $(a,b)$ with $a,b > 0$ and $a^2+b^2 > 1$. Consider the minimization problem:
\begin{align*}
\text{minimize } &f(x,y) := (x-a)^2 + (y-b)^2 \\
\text{subject to } &g_1(x,y) := x^2+y^2-1 \leq 0
\end{align*}
Our intuition says that the minimizer should be $\left( \frac{a}{\sqrt{a^2+b^2}}, \frac{b}{\sqrt{a^2+b^2}} \right)$. We have $\nabla g(x,y) = (2x, 2y)$, so clearly all feasible points are regular. The Kuhn-Tucker conditions are
\[
\begin{pmatrix}
2(x-a) \\ 2(y-a)
\end{pmatrix} + \mu \begin{pmatrix}
2x \\ 2y
\end{pmatrix} = \begin{pmatrix}
0 \\ 0
\end{pmatrix}
\]
and $\mu g(x,y) = 0$. That is,
\begin{align*}
(1+\mu)x &= a \\
(1+\mu)y &= b \\
\mu (x^2+y^2 - 1) &= 0, \mu \geq 0
\end{align*}
Suppose $\mu = 0$. Then $x = a$ and $y = b$; since we assumed $a^2+b^2 > 1$, we would have that $(x,y)$ is not feasible. Therefore $\mu \neq 0$, and so $x^2+y^2 = 1$ by the third equation. Squaring the first two equations and adding them gives
\[
(1+\mu)^2 (x^2+y^2) = a^2+b^2,
\]
implying that $\mu = -1 + \sqrt{a^2+b^2}$ - we took the positive root because $\mu > 0$. This is actually positive, since $a^2+b^2 > 1$. Those first equations again give us
\[
\begin{pmatrix}
x_0 \\ y_0
\end{pmatrix} = \frac{1}{1+\mu} \begin{pmatrix}
a \\ b
\end{pmatrix} = \frac{1}{\sqrt{a^2+b^2}} \begin{pmatrix}
a \\ b
\end{pmatrix},
\]
as expected. What do the second order conditions tell us? The Lagrangian is
\[
L(x_0, y_0) = 2I + 2\mu I = 2(1+\mu)I = 2\sqrt{a^2+b^2} I,
\]
which is everywhere positive definite. Therefore the second-order sufficient conditions are satisfied. For practice, however, let's compute the tangent space to the "strongly active constraints". The only constraint is $g$; since $g$ is active and its Lagrange multiplier $\mu$ is positive, the constraint $g$ is strongly active at $(x_0, y_0)$. Therefore the tangent space we are interested in is the tangent space to $S^1$ at $(x_0, y_0)$: that space is $\{v \in \R^2 : av_1 + bv_2 = 0\}$.

\item
Consider the problem
\begin{align*}
\text{minimize } &f(x,y) := x^3 + y^2 \\
\text{subject to } &g(x,y) := (x+1)^2 + y^2 - 1 \leq 0.
\end{align*}
We have $\nabla g(x,y) = (2(x+1), 2y)$, which makes it clear that every feasible point is regular. The Kuhn-Tucker conditions are
\[
\begin{pmatrix}
3x^2 \\ 2y
\end{pmatrix} + \mu \begin{pmatrix}
2(x+1) \\ 2y
\end{pmatrix} = \begin{pmatrix}
0 \\ 0
\end{pmatrix},
\]
with $\mu((x+1)^2 + y^2 - 1) = 0$ and $\mu \geq 0$.

Consider $(x_0, y_0) = (0,0)$. The Kuhn-Tucker conditions imply $\mu = 0$. In particular, $g$ is active at $(0,0)$, but not strongly active there. The tangent space to the active constraint at $(0,0)$ is the y-axis. The Lagrangian at $(0,0)$ is
\[
L(0,0) = \begin{pmatrix}
0 & 0 \\ 0 & 2
\end{pmatrix},
\]
which is clearly positive definite on this tangent space. However, we cannot conclude anything, since the constraint $g$ is not strongly active. In fact, it is clear that $(0,0)$ is not a local minimizer: for $x<0$ sufficiently close to $0$, $f(x,0)$ is negative, yet it is $0$ at $(0,0)$.
\end{enumerate}

\end{document}
